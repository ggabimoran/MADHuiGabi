---
title: "Mélange de Bernoulli"
author: "Jingzhuo HUI, Gabriel Moran"
date: "24/10/2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modèle

Considérons un vecteur aléatoire binaire $\boldsymbol{x} \in [0,1]^p$ de $p$ variables $x_j$ suivant chacune
une distribution de Bernoulli $\mathcal{B}(\mu_j)$. La distribution du vecteur s'exprime comme:
$$
p(\boldsymbol{x}| \boldsymbol{\mu}) = \prod_{j=1}^p \mu_j^{x_j} (1-\mu_j)^{1-x_j}, 
$$
avec $\boldsymbol{x}=(x_1, \cdots, x_p)^T$ et  $\boldsymbol{\mu}=(\mu_1, \cdots, \mu_p)^T$.

Soit une distribution  mélange à $K$ composantes  de Bernoulli
$$
p(\boldsymbol{x} | \boldsymbol{\pi}, \boldsymbol{M}) = \sum_{k=1}^K
                  \pi_k p(\boldsymbol{x} | \boldsymbol{\mu}_k)
$$
où les $\pi_k$ sont les proportions du mélange et les $p(\boldsymbol{x} | \boldsymbol{\mu}_k)$ sont des distributions de Bernoulli multivariées de
paramètres  $\boldsymbol{\mu}_k=(\mu_{k1}, \cdots, \mu_{kp})^T$, et $M=\{\boldsymbol{\mu}_1, \cdots , \boldsymbol{\mu}_K\}^T$
la matrice des paramètres des densités de classes.

Dans la suite nous considérerons
\begin{itemize}
\item un échantillon observé $X = \{\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n\}$ issu de cette distribution mélange, 
\item des  variables latentes $Z=\{z_1, \cdots, z_n\}$ indiquant la composante d'origine de chaque $\boldsymbol{x}_i$.  
\end{itemize}

```{r , echo=FALSE,  warning=FALSE, error=FALSE, include=FALSE}
library(tidyverse)
library(reshape2)
```
  
## Exercice 1

Considérons un mélange à 3 composantes de Bernoulli mélangées en proportions égales $\pi_1 = \pi_2 = \pi_3$.

Simulons une matrice $M$ de proportions dont les $3$ lignes et les $50$ colonnes décrivent $3$ vecteurs des proportions d'un mélange de Bernoulli dans un espace de dimension $50$.

```{r}
set.seed(3)
K<-3
p<-50
n<-200
pi<-matrix(c(1/3,1/3,1/3))
M<-matrix(runif(K*p),K,p)
M[K,]<-1-M[1,]
```

Simulons $Z=\{z_1, \cdots, z_n\}$ pour $n=200$.

```{r}
nks<-rmultinom(1,200,prob = pi)
Z<-apply(diag(x=1,K,K),MARGIN = 2,FUN=rep,times=nks)
```

Simulons $X|Z$.

```{r}
X <-do.call(rbind, 
                  mapply(function(nk,k){
                    matrix(rbernoulli(nk*p,p=M[k,]),
                           nrow = nk,
                           ncol=p,
                           byrow = TRUE)}, nks,1:K))
```

Permutons les lignes de la matrice $X$ à $200$ lignes et $50$ colonnes et visualisons la matrice ainsi obtenue. 

```{r}
permutation <- sample(nrow(X))
X <- X[permutation,]
ggplot(melt(X), aes(x = Var2, y = Var1)) + 
  geom_raster(aes(fill=value)) + 
  scale_fill_brewer(aesthetics = "fill") + 
  labs(x="Variables", y="Individus", title="Matrix")
```

On observe que l'échantillon a bien été mélangé et qu'il est impossible de distinguer graphiquement 3 motifs. On va désormais appliquer l'algorithme des kmeans à $X$ avec 3 classes et produire la matrice $Z$ des variables latentes correspondantes.

```{r}
kmeans(X,3,nstart = 10)->res.kmeans
Z_kmeans<-matrix(0,nrow=n,ncol=K)
for (j in 1:K){
  Z_kmeans[res.kmeans$cluster==j,j]<-1
}
```

On remarque déjà que la taille des clusters obtenues correspond bien celles des composantes de notre mélange de Bernoulli (donnée par nks) :

```{r}
c(nks)
res.kmeans$size
```

L'algorithme des kmeans convergeant vers un minimum local, on choisi nstart=10 initialisations différentes pour optimiser la réponse. L'algorithme des kmeans minimise l'inertie intra-classe et maximise l'inertie inter-classes. En assignant l'échantillon à $3$ classes au lieu d'une seule, la réduction de variance expliquée par les clusters vaut (en pourcentage):

```{r}
100*res.kmeans$tot.withinss/res.kmeans$totss
```

Visualisons la matrice classée :

```{r}
tidyData<-melt(X[order(res.kmeans$cluster),order(M[1,])])

ggplot(tidyData, aes(x = Var2, y = Var1)) + 
  geom_raster(aes(fill=value)) + 
  scale_fill_brewer(aesthetics = "fill") + 
  labs(x="Variables", y="Individus", title="Matrix") 
```

On observe bien les 3 motifs distincts.


##Exercice 2

Q1. Calculons la log-vraisemblance complète $\ln P(\mathbf{X},\mathbf{Z}|\boldsymbol{\theta}=\{\boldsymbol{\pi},\boldsymbol{M}\})$.

Calculons d'abord la vraisemblance complète. On a :

$P(\mathbf{X}, \mathbf{Z}| \mathbf{\pi}, \mathbf{M}) = P(\mathbf{X} | \mathbf{Z}, \mathbf{\pi}, \mathbf{M}) P(\mathbf{Z}| \mathbf{\pi}, \mathbf{M})$

Or, les observations étant indépendantes, en notant $z_{i, k}=\mathbf{1}_{Z_i=k}$ : 
$P(\mathbf{X}|\mathbf{Z}, \mathbf{\pi}, \mathbf{M}) = \prod_{i = 1}^n P(\mathbf{x_i}|\mathbf{z_i},\mathbf{\pi},\mathbf{M}) = \prod_{i = 1}^n \prod_{k = 1}^K P(\mathbf{x_i} | \mathbf{\mu_k})^{z_{i, k}} = \prod_{i = 1}^n \prod_{k = 1}^K \left( \prod_{j = 1}^p \mu_{k, j}^{x_{i, j}} (1 - \mu_{k, j})^{1 - x_{i, j}} \right)^{z_{i, k}}$

Par ailleurs, comme $\pi_k=P(z_i=k)$,on a:
$P(\mathbf{Z}|\mathbf{\pi}) = \prod_{i = 1}^n \,P(\mathbf{z_i}|\mathbf{\pi}) = \prod_{i = 1}^n \prod_{k = 1}^K \pi_k^{z_{i, k}}$

On obtient donc pour la vraisemblance complète :

$P(\mathbf{X}, \mathbf{Z}| \mathbf{\pi}, \mathbf{M}) = \prod_{i = 1}^n \prod_{k = 1}^K \left( \pi_k \prod_{j = 1}^p \mu_{k, j}^{x_{i, j}} (1 - \mu_{k, j})^{1 - x_{i, j}} \right)^{z_{i, k}}$

En prenant le logarithme, cela donne :

$\ln P(\mathbf{X}, \mathbf{Z}| \mathbf{\pi}, \mathbf{M}) = \sum_{i = 1}^n \sum_{k = 1}^K z_{i, k} \left( \ln \pi_k + \sum_{j = 1}^p x_{i, j} \ln \mu_{k, j} + (1 - x_{i, j}) \ln (1 - \mu_{k, j}) \right)$

Q2. Calculons $t_{ik}^q=\mathbb{E}[z_{ik}]$ par rapport à la loi $p_{\theta^{q}}(\mathbf{Z}|\mathbf{X})$. On a, en utilisant le théorème de Bayes et le fait que $P(\mathbf{x_i}|z_{i,k})=P(\mathbf{x_i}|\mu_k)$ :

$t_{ik}^q=\mathbb{E}[z_{i, k}] = P(z_{i,k} | \mathbf{x_i}, \mathbf{\pi}, \mathbf{M}) = \frac{P(z_{i,k}) P(\mathbf{x_i} |\mathbf{\mu_k})}{P(\mathbf{x_i})}$.

Puis, par la formule des probabilités totales :

$$t_{i,k}^q= \frac{\pi_k P(\mathbf{x_i} |\mathbf{\mu_k})}{\sum_{m = 1}^K \pi_m P(\mathbf{x_i} | \mathbf{\mu_m})} = \frac{\pi_k \prod_{j = 1}^p \mu_{k, j}^{x_{i, j}} (1 - \mu_{k, j})^{1 - x_{i, j}} }{\sum_{m = 1}^K \pi_m \prod_{j = 1}^p \mu_{m, j}^{x_{i, j}} (1 - \mu_{m, j})^{1 - x_{i, j}}}$$

Q.3 On en déduit $Q(\theta^q|\theta)$, l'espérance de cette log-vraisemblance par rapport à la loi $p_{\theta^{q}}(\mathbf{Z}|\mathbf{X})$ :

$Q(\theta^q|\theta)=\mathbb{E}[\ln P(\mathbf{X}, \mathbf{Z}| \mathbf{\pi}, \mathbf{M})] = \sum_{i = 1}^n \sum_{k = 1}^K t_{i,k}^q \left( \ln \pi_k + \sum_{j = 1}^p x_{i, j} \ln \mu_{k, j} + (1 - x_{i, j}) \ln (1 - \mu_{k, j}) \right)$

Q.4 Pour déterminer $\theta^{q+1}=argmax_\theta Q(\theta^q|\theta)$, on peut commencer par maximiser l'argument par rapport à $\boldsymbol{\mu}_k$ en annulant sa dérivée :

$\frac{\partial}{\partial \mu_{k, j}} Q(\theta^q|\theta) = \sum_{i = 1}^n t_{i,k}^q ( \frac{x_{i, j}}{\mu_{k, j}} - \frac{1 - x_{i, j}}{1 - \mu_{k, j}}) = \sum_{i = 1}^n t_{i,k}^q \frac{x_{i, j} - \mu_{k, j}}{\mu_{k, j} (1 - \mu_{k, j})} = 0 \Leftrightarrow \mu_{k, j} = \frac{1}{n_k} \sum_{i = 1}^n x_{i, j} t_{i,k}^q$ 

où $n_k=\sum_{i = 1}^n t_{i,k}^q$ correspond au nombre de points affectés au cluster $k$. Ainsi, pour $k \in \{1,...K\}$, le maximum de $Q(\theta^q|\theta)$ par rapport à $\boldsymbol{\mu}_k$ est :

$$\boldsymbol{\mu}_k=\frac{1}{n_k} \sum_{i = 1}^n \boldsymbol{x}_i t_{i,k}^q$$

Pour maximiser la fonction $Q(\theta^q|\theta)$ par rapport à $\pi$, sous la contrainte $\sum_{k=1}^K \pi_k=1$, on peut introduire le multiplicateur de Lagrange. Le problème d'optimisation devient alors de maximiser la fonction $\Lambda(\theta,\lambda)= Q(\theta^q|\theta)+\lambda (\sum_{k=1}^K \pi_k - 1)$. En dérivant succéssivement par rapport à $\pi_k$ puis $\lambda$, on obtient : 
$\frac{\partial}{\partial \pi_{k}} \Lambda(\theta, \lambda) = \frac{1}{\pi_k} \sum_{i = 1}^n t_{i,k}^q + \lambda = 0 \Leftrightarrow \pi_k = -\frac{n_k}{\lambda}$,

$\frac{\partial}{\partial \lambda} \Lambda(\theta, \lambda) = \sum_{k = 1}^K \pi_k - 1 = 0 \Leftrightarrow \sum_{k = 1}^K \pi_k = 1$.

En combinant ces deux résultats, on obtient $\lambda = - \sum_{k = 1}^K n_k = - n$ et donc : $$\pi_k = -\frac{n_k}{\lambda} = \frac{n_k}{n}$$

Q.5 On commence l'algorithme EM en initialisant $\theta$. Ensuite, l'étape E de l'algorithme EM consiste à calculer les $t_{i,k}^q=\mathbb{E}[z_{ik}]$ pour mettre à jour la distribution des variables latentes $z_i$. Puis l'étape M consiste à affecter à $\theta$ le maximum de $Q(\theta^q|\theta)=\mathbb{E}[\ln P(\mathbf{X}, \mathbf{Z}| \mathbf{\pi}, \mathbf{M})]$ par rapport à $\theta$. Le $\theta$ estimé est obtenu lors de la convergence.

Q.6

##Exercice 3

Q.1 Ecrivons une fonction E-step qui produit les $t_{i,k}$ à partir de $\Theta$.
```{r}
E_Step<-function(X_,M_,pi_){
  #Calcul de P(X|M)
  p_x_mu <- matrix(0,nrow=n,ncol=K)
  for (i in 1:n){
    for (k in 1:K){
      p_x_mu[i,k]<-prod(M_[k,]^X_[i,]*(1-M_[k,])^(1-X_[i,]))
    }
  }
  #Calcul de T la matrice des t_i,k
  T<-matrix(0,nrow=n,ncol = K)
  for (i in 1:n){
    for (k in 1:K){
      T[i,k]<-pi_[k]*p_x_mu[i,k]/(t(pi_)%*%p_x_mu[i,])
    }
  }
  return (T)
}
```

Nous voulons vérifier les résultats en injectant les vrais paramètres de notre simulation et en comparant les $t_{i,k}$ estimé par rapport aux variables latentes Z de notre simulation (ce qui correspond à notre Z_kmeans). Si contrairement aux variables latentes $z_1,..,z_n$, les $t_i$ ne sont pas des variables binaires, on s'attend à avoir le même maximum de responsabilité pour chaque $x_i$, $i \in \{1,..,n\}$. On peut donc montrer dans un premier temps que les $t_i$ et $z_i$ ont le même maximum de responsabilité, puis, dans un second temps, on peut comparer la norme de la différence des deux matrices (qui doit être "proche", au sens de la norme matricielle, de la matrice nulle). Or, on ne sait pas dans quel ordre l'algorithme kmeans numérote les clusters et donc si les numérotations des composantes et des clusters correspondent. On va donc devoir comparer pour chaque permutation de la numérotation des clusters. Dès que tout les $t_i$ et $z_i$ ont le même maximum de responsabilité, on sait que l'on aura obtenu la bonne numérotation.

```{r,warning=FALSE}
T<-E_Step(X,M,pi)
Tmax<-apply(T,which.max,MARGIN = 1)
library(combinat)
cluster_permutations <- permn(1:K)
for (i in 1:K){
  copycluster<-res.kmeans$cluster
  for (k in 1:K){
    copycluster[res.kmeans$cluster==k]<-cluster_permutations[[i]][k]
  }
  if(sum(copycluster==Tmax)==n){
    print("Mêmes maximums de responsabilité pour tout x_i")
    print("Norme de la différence des matrices Z et T:")
    print(norm(T[,cluster_permutations[[i]]]-Z_kmeans))
    break;
  }
}
```

On observe que la valeur de la norme obtenue est proche de 0, ce qui est cohérent.

Q.2 Ecrivons une fonction M-step qui produit les estimateurs des $\Theta$ à partir des données observées et des $t_{i,k}$. On rappelle les résultats de l'exercice 2 :

$$\boldsymbol{\mu}_k=\frac{1}{n_k} \sum_{i = 1}^n \boldsymbol{x}_i t_{i,k}^q$$
$$\pi_k = \frac{n_k}{n}$$

où $n_k=\sum_{i = 1}^n t_{i,k}^q$

```{r}
M_Step<-function(X_,T_){
  #Calcul des mu_k
  new_mu <- matrix(1:K,ncol = 1)
  new_mu <- t(apply(new_mu,MARGIN=1,function(k){colSums(X_*c(T_[,k]))/colSums(T_)[k]}))
  #Calcul des pi_k
  new_pi <- colSums(T_)/n
  return (list("new_pi"=new_pi,"new_mu"=new_mu))
}
```

De manière analogue à la question précédente, on peut tester la fonction sur notre simulation dont on connait tous les paramètres et calculer les normes des différences pour voir si elles approchent la matrice nulle : 

```{r}
new_theta <- M_Step(X,T)
print("Norme de la différence des matrices M et new_mu:")
print(norm(M-new_theta$new_mu))
print("Norme de la différence des matrices pi et new_pi:")
print(norm(pi-new_theta$new_pi))
```

On obtient des normes proches de 0, ce qui est cohérent.

Q.3













